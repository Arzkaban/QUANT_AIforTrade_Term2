{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T18:47:16.873248Z",
     "start_time": "2021-01-24T18:47:14.130650Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ltjsu\\.conda\\envs\\zipline_env\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import project_helper\n",
    "import project_tests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "import os\n",
    "from zipline.data import bundles\n",
    "os.environ['ZIPLINE_ROOT'] = os.path.join('C:/Users/ltjsu/.zipline')\n",
    "ingest_func = bundles.csvdir.csvdir_equities(['daily'], 'custom-csvdir-bundle')\n",
    "bundles.register('custom-csvdir-bundle', ingest_func)\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "from zipline.utils.calendars import get_calendar\n",
    "from zipline.data.data_portal import DataPortal\n",
    "from zipline.pipeline.factors import CustomFactor, DailyReturns, Returns, SimpleMovingAverage, AnnualizedVolatility\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "from scipy.stats import spearmanr\n",
    "from IPython.display import display\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import alphalens as al\n",
    "import abc\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import Bunch\n",
    "sector_lookup = pd.read_csv(\n",
    "    os.path.join(os.getcwd(), 'data', 'project_7_sector', 'labels.csv'),\n",
    "    index_col='Sector_i')['Sector'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T18:47:16.910291Z",
     "start_time": "2021-01-24T18:47:16.873248Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close'):\n",
    "    end_dt = pd.Timestamp(end_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "    start_dt = pd.Timestamp(start_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    return data_portal.get_history_window(\n",
    "        assets=assets,\n",
    "        end_dt=end_dt,\n",
    "        bar_count=end_loc - start_loc,\n",
    "        frequency='1d',\n",
    "        field=field,\n",
    "        data_frequency='daily')\n",
    "\n",
    "def momentum_1yr(window_length, universe, sector):\n",
    "    return Returns(window_length=window_length, mask=universe) \\\n",
    "        .demean(groupby=sector) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "\n",
    "def mean_reversion_5day_sector_neutral_smoothed(window_length, universe, sector):\n",
    "    unsmoothed_factor = -Returns(window_length=window_length, mask=universe) \\\n",
    "        .demean(groupby=sector) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "    return SimpleMovingAverage(inputs=[unsmoothed_factor], window_length=window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "\n",
    "class CTO(Returns):\n",
    "    \"\"\"\n",
    "    Computes the overnight return, per hypothesis from\n",
    "    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554010\n",
    "    \"\"\"\n",
    "    inputs = [USEquityPricing.open, USEquityPricing.close]\n",
    "    \n",
    "    def compute(self, today, assets, out, opens, closes):\n",
    "        \"\"\"\n",
    "        The opens and closes matrix is 2 rows x N assets, with the most recent at the bottom.\n",
    "        As such, opens[-1] is the most recent open, and closes[0] is the earlier close\n",
    "        \"\"\"\n",
    "        out[:] = (opens[-1] - closes[0]) / closes[0]\n",
    "        \n",
    "class TrailingOvernightReturns(Returns):\n",
    "    \"\"\"\n",
    "    Sum of trailing 1m O/N returns\n",
    "    \"\"\"\n",
    "    window_safe = True\n",
    "    \n",
    "    def compute(self, today, asset_ids, out, cto):\n",
    "        out[:] = np.nansum(cto, axis=0)\n",
    "\n",
    "def overnight_sentiment_smoothed(cto_window_length, trail_overnight_returns_window_length, universe):\n",
    "    cto_out = CTO(mask=universe, window_length=cto_window_length)\n",
    "    unsmoothed_factor = TrailingOvernightReturns(inputs=[cto_out], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "    return SimpleMovingAverage(inputs=[unsmoothed_factor], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "        \n",
    "class MarketDispersion(CustomFactor):\n",
    "    inputs = [DailyReturns()]\n",
    "    window_length = 1\n",
    "    window_safe = True\n",
    "\n",
    "    def compute(self, today, assets, out, returns):\n",
    "        # returns are days in rows, assets across columns\n",
    "        out[:] = np.sqrt(np.nanmean((returns - np.nanmean(returns))**2))\n",
    "        \n",
    "class MarketVolatility(CustomFactor):\n",
    "    inputs = [DailyReturns()]\n",
    "    window_length = 1\n",
    "    window_safe = True\n",
    "    \n",
    "    def compute(self, today, assets, out, returns):\n",
    "        mkt_returns = np.nanmean(returns, axis=1)\n",
    "        out[:] = np.sqrt(260.* np.nanmean((mkt_returns-np.nanmean(mkt_returns))**2))\n",
    "        \n",
    "def sp(group, col1_name, col2_name):\n",
    "    x = group[col1_name]\n",
    "    y = group[col2_name]\n",
    "    return spearmanr(x, y)[0]\n",
    "\n",
    "def train_test_split(all_x, all_y, Test_Date):\n",
    "    \"\"\"\n",
    "    Generate the train and test dataset.\n",
    "    \"\"\"\n",
    "    train_inx_first = int(all_x.reset_index()['level_0'].drop_duplicates()\n",
    "                          .loc[all_x.reset_index()['level_0'].drop_duplicates() == Test_Date, ].index[0])\n",
    "    train_inx_last  = int(all_x.reset_index()['level_0'].drop_duplicates(keep='last')\n",
    "                          .loc[all_x.reset_index()['level_0'].drop_duplicates(keep='last') == Test_Date, ].index[0])\n",
    "    x_train = all_x[:train_inx_first]\n",
    "    x_test  = all_x[train_inx_first:(train_inx_last+1)]\n",
    "    y_train = all_y[:train_inx_first]\n",
    "    y_test  = all_y[train_inx_first:(train_inx_last+1)]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def get_IC(factor_data):\n",
    "    ls_IC = pd.DataFrame()\n",
    "    \n",
    "    for factor, factor_data in factor_data.items():\n",
    "        ls_IC[factor] = al.performance.factor_information_coefficient(factor_data).iloc[:,0]\n",
    "        print('*************************************\\n#####################################\\n',factor,'\\n')\n",
    "        al.tears.create_full_tear_sheet(factor_data)\n",
    "        \n",
    "    return ls_IC\n",
    "\n",
    "def Alpha_Score(data, samples, classifier, factors,max_loss=0.35):\n",
    "    # Calculate the Alpha Score\n",
    "    prob_array=[-1,1]\n",
    "    alpha_score = classifier.predict_proba(samples).dot(np.array(prob_array))\n",
    "    return alpha_score\n",
    "\n",
    "def non_overlapping_samples(x, y, n_skip_samples, start_i=0):\n",
    "    \"\"\"\n",
    "    Get the non overlapping samples.\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    \n",
    "    # TODO: Implement\n",
    "    subsampled_idx = x.index.levels[0].tolist()[start_i :: n_skip_samples + 1]\n",
    "#     subsampled_idx = (all_factors.weekday == start_i)\n",
    "    non_overlapping_x = x.loc[subsampled_idx]\n",
    "    non_overlapping_y = y.loc[subsampled_idx]\n",
    "    \n",
    "    return non_overlapping_x, non_overlapping_y\n",
    "\n",
    "class NoOverlapVoterAbstract(VotingClassifier):\n",
    "    @abc.abstractmethod\n",
    "    def _calculate_oob_score(self, classifiers):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def _non_overlapping_estimators(self, x, y, classifiers, n_skip_samples):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __init__(self, estimator, voting='soft', n_skip_samples=9):\n",
    "        # List of estimators for all the subsets of data\n",
    "        estimators = [('clf'+str(i), estimator) for i in range(n_skip_samples + 1)]\n",
    "        \n",
    "        self.n_skip_samples = n_skip_samples\n",
    "        super().__init__(estimators, voting)\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        estimator_names, clfs = zip(*self.estimators)\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        \n",
    "        clone_clfs = [clone(clf) for clf in clfs]\n",
    "        self.estimators_ = self._non_overlapping_estimators(X, y, clone_clfs, self.n_skip_samples)\n",
    "        self.named_estimators_ = Bunch(**dict(zip(estimator_names, self.estimators_)))\n",
    "        self.oob_score_ = self._calculate_oob_score(self.estimators_)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "def calculate_oob_score(classifiers):\n",
    "    \"\"\"\n",
    "    Calculate the mean out-of-bag score from the classifiers.\n",
    "    \"\"\"  \n",
    "    # TODO: Implement\n",
    "    oob_score = 0\n",
    "    for clf in classifiers:\n",
    "        oob_score = oob_score + clf.oob_score_\n",
    "    return oob_score / len(classifiers)\n",
    "\n",
    "def non_overlapping_estimators(x, y, classifiers, n_skip_samples):\n",
    "    \"\"\"\n",
    "    Fit the classifiers to non overlapping data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    fit_classifiers = []\n",
    "    for start_idx, clf in enumerate(classifiers):\n",
    "        X_resampled, y_resampled = non_overlapping_samples(x, y, n_skip_samples, start_idx)\n",
    "        fit_classifiers.append(clf.fit(X_resampled, y_resampled))\n",
    "        \n",
    "    return fit_classifiers\n",
    "\n",
    "class NoOverlapVoter(NoOverlapVoterAbstract):\n",
    "    def _calculate_oob_score(self, classifiers):\n",
    "        return calculate_oob_score(classifiers)\n",
    "        \n",
    "    def _non_overlapping_estimators(self, x, y, classifiers, n_skip_samples):\n",
    "        return non_overlapping_estimators(x, y, classifiers, n_skip_samples)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T19:01:03.765306Z",
     "start_time": "2021-01-24T18:47:23.265023Z"
    }
   },
   "outputs": [],
   "source": [
    "trading_calendar = get_calendar('XSHG')\n",
    "bundle_data = bundles.load('custom-csvdir-bundle')\n",
    "engine = project_helper.build_pipeline_engine(bundle_data, trading_calendar)\n",
    "universe_end_date = pd.Timestamp('2020-10-27', tz='UTC')\n",
    "# 2020-10-27;2020-09-28;2020-08-28;2020-07-28;2020-06-24;2020-05-28;2020-04-28;2020-03-27;2020-02-28(3);2020-01-23(4);\n",
    "# 2019-12-27;2019-11-28;2019-10-28;2019-09-27;2019-08-28;2019-07-26\n",
    "# factor_start_date = universe_end_date - pd.DateOffset(years=6, days=1)\n",
    "factor_start_date = pd.Timestamp('2014-07-22', tz='UTC')\n",
    "\n",
    "bundle_data.equity_daily_bar_reader\n",
    "\n",
    "universe = AverageDollarVolume(window_length=120).top(500)\n",
    "sector = project_helper.Sector()\n",
    "\n",
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(momentum_1yr(252, universe, sector),'Momentum_1YR')\n",
    "pipeline.add(mean_reversion_5day_sector_neutral_smoothed(20, universe, sector),'Mean_Reversion_Sector_Neutral_Smoothed')\n",
    "pipeline.add(overnight_sentiment_smoothed(2, 10, universe),'Overnight_Sentiment_Smoothed')\n",
    "pipeline.add(AnnualizedVolatility(window_length=20, mask=universe).rank().zscore(), 'volatility_20d')\n",
    "pipeline.add(AnnualizedVolatility(window_length=120, mask=universe).rank().zscore(), 'volatility_120d')\n",
    "pipeline.add(AverageDollarVolume(window_length=20, mask=universe).rank().zscore(), 'adv_20d')\n",
    "pipeline.add(AverageDollarVolume(window_length=120, mask=universe).rank().zscore(), 'adv_120d')\n",
    "pipeline.add(sector, 'sector_code')\n",
    "pipeline.add(SimpleMovingAverage(inputs=[MarketDispersion(mask=universe)], window_length=20), 'dispersion_20d')\n",
    "pipeline.add(SimpleMovingAverage(inputs=[MarketDispersion(mask=universe)], window_length=120), 'dispersion_120d')\n",
    "pipeline.add(MarketVolatility(window_length=20), 'market_vol_20d')\n",
    "pipeline.add(MarketVolatility(window_length=120), 'market_vol_120d')\n",
    "pipeline.add(Returns(window_length=10, mask=universe).quantiles(2), 'return_10d')\n",
    "\n",
    "all_factors = engine.run_pipeline(pipeline, factor_start_date, universe_end_date)\n",
    "all_factors['is_Janaury'] = all_factors.index.get_level_values(0).month == 1\n",
    "all_factors['is_December'] = all_factors.index.get_level_values(0).month == 12\n",
    "all_factors['weekday'] = all_factors.index.get_level_values(0).weekday\n",
    "all_factors['quarter'] = all_factors.index.get_level_values(0).quarter\n",
    "all_factors['qtr_yr'] = all_factors.quarter.astype('str') + '_' + all_factors.index.get_level_values(0).year.astype('str')\n",
    "all_factors['month_end'] = all_factors.index.get_level_values(0).isin(\n",
    "                            pd.date_range(start=factor_start_date, end=universe_end_date, freq='BM'))\n",
    "all_factors['month_start'] = all_factors.index.get_level_values(0).isin(\n",
    "                            pd.date_range(start=factor_start_date, end=universe_end_date, freq='BMS'))\n",
    "all_factors['qtr_end'] = all_factors.index.get_level_values(0).isin(\n",
    "                            pd.date_range(start=factor_start_date, end=universe_end_date, freq='BQ'))\n",
    "all_factors['qtr_start'] = all_factors.index.get_level_values(0).isin(\n",
    "                            pd.date_range(start=factor_start_date, end=universe_end_date, freq='BQS'))\n",
    "\n",
    "all_factors['return_10d'].replace(to_replace=-1, value=np.nan, inplace=True)\n",
    "\n",
    "sector_columns = []\n",
    "for sector_i, sector_name in sector_lookup.items():\n",
    "    secotr_column = '{}行业'.format(sector_name)\n",
    "    sector_columns.append(secotr_column)\n",
    "    all_factors[secotr_column] = (all_factors['sector_code'] == sector_i)\n",
    "all_factors['target_10D'] = all_factors.groupby(level=1)['return_10d'].shift(-10)\n",
    "\n",
    "features = [\n",
    "    'Mean_Reversion_Sector_Neutral_Smoothed', 'Momentum_1YR',\n",
    "    'Overnight_Sentiment_Smoothed', 'adv_120d', 'adv_20d',\n",
    "    'dispersion_120d', 'dispersion_20d', 'market_vol_120d',\n",
    "    'market_vol_20d', 'volatility_20d',\n",
    "    'is_Janaury', 'is_December', 'weekday',\n",
    "    'month_end', 'month_start', 'qtr_end', 'qtr_start'] + sector_columns\n",
    "target_label = 'target_10D'\n",
    "temp = all_factors.dropna().copy()\n",
    "X = temp[features]\n",
    "y = temp[target_label].astype(int)\n",
    "\n",
    "factor_names = [\n",
    "    'Mean_Reversion_Sector_Neutral_Smoothed',\n",
    "    'Momentum_1YR',\n",
    "    'Overnight_Sentiment_Smoothed'\n",
    "    ]\n",
    "clf_random_state = 0\n",
    "n_days = 10\n",
    "n_stocks = 500\n",
    "clf_parameters = {\n",
    "    'criterion': 'entropy','min_samples_leaf': n_stocks * n_days,\n",
    "    'oob_score': True,'n_jobs': -1, 'random_state': clf_random_state}\n",
    "n_trees = 500\n",
    "n_skip_samples = 9\n",
    "AlphaScores = []\n",
    "Dates = X.reset_index()['level_0'].drop_duplicates().values\n",
    "i =0\n",
    "X_test_F = pd.DataFrame()\n",
    "Y_test_F = pd.DataFrame()\n",
    "for Date in Dates[1223:]:\n",
    "    if (i >= 22)|(Date == Dates[-1]):\n",
    "        print(Date)\n",
    "        i = 0\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, Date)\n",
    "        X_test_F = X_test_F.append(X_test)\n",
    "        Y_test_F = Y_test_F.append(y_test)\n",
    "        clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "        clf_nov = NoOverlapVoter(clf, n_skip_samples =n_skip_samples)\n",
    "        y_train\n",
    "        clf_nov.fit(X_train, y_train)\n",
    "        AlphaScore = Alpha_Score(all_factors, X_test_F, clf_nov, factor_names)\n",
    "        AlphaScores = np.hstack((AlphaScores,AlphaScore))\n",
    "        X_test_F = pd.DataFrame()\n",
    "        Y_test_F = pd.DataFrame()\n",
    "    else:\n",
    "        i = i + 1\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, Date)\n",
    "        X_test_F = X_test_F.append(X_test)\n",
    "        Y_test_F = Y_test_F.append(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T19:03:43.438004Z",
     "start_time": "2021-01-24T19:02:42.573824Z"
    }
   },
   "outputs": [],
   "source": [
    "def Get_Date(all_x, all_y, Start_Date):\n",
    "    \"\"\"\n",
    "    Generate the train and test dataset.\n",
    "    \"\"\"\n",
    "    Start_inx = int(all_x.reset_index()['level_0'].drop_duplicates()\n",
    "                    .loc[all_x.reset_index()['level_0'].drop_duplicates() == Start_Date, ].index[0])\n",
    "#     End_inx = int(all_x.reset_index()['level_0'].drop_duplicates()\n",
    "#                     .loc[all_x.reset_index()['level_0'].drop_duplicates() == End_Date, ].index[0])\n",
    "    X_Train  = all_x[Start_inx:]\n",
    "    y_Train  = all_y[Start_inx:]\n",
    "#     X_Test  = all_x[End_inx:]\n",
    "#     y_Test  = all_y[End_inx:]\n",
    "    return X_Train, y_Train\n",
    "\n",
    "data_portal = DataPortal(\n",
    "    bundle_data.asset_finder,\n",
    "    trading_calendar=trading_calendar,\n",
    "    first_trading_day=bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "    equity_minute_reader=None,\n",
    "    equity_daily_reader=bundle_data.equity_daily_bar_reader,\n",
    "    adjustment_reader=bundle_data.adjustment_reader)\n",
    "\n",
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close'):\n",
    "    end_dt = pd.Timestamp(end_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "    start_dt = pd.Timestamp(start_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    return data_portal.get_history_window(\n",
    "        assets=assets,\n",
    "        end_dt=end_dt,\n",
    "        bar_count=end_loc - start_loc,\n",
    "        frequency='1d',\n",
    "        field=field,\n",
    "        data_frequency='daily')\n",
    "all_assets = all_factors.index.levels[1].values.tolist()\n",
    "all_pricing = get_pricing(\n",
    "    data_portal,\n",
    "    trading_calendar,\n",
    "    all_assets,\n",
    "    factor_start_date,\n",
    "    universe_end_date)\n",
    "\n",
    "def get_IC(factor_data):\n",
    "    ls_IC = pd.DataFrame()\n",
    "    \n",
    "    for factor, factor_data in factor_data.items():\n",
    "        ls_IC[factor] = al.performance.factor_information_coefficient(factor_data).iloc[:,0]\n",
    "        print('*************************************\\n#####################################\\n',factor,'\\n')\n",
    "        al.tears.create_full_tear_sheet(factor_data)\n",
    "        \n",
    "    return ls_IC\n",
    "\n",
    "def build_factor_data(factor_data, pricing,max_loss=0.35,periods=[1,5,10]):\n",
    "    return {factor_name: al.utils.get_clean_factor_and_forward_returns(factor=data, prices=pricing,max_loss=max_loss, periods=periods)\n",
    "        for factor_name, data in factor_data.iteritems()}\n",
    "\n",
    "def show_sample_results(data, samples, factors,alpha_score, pricing=all_pricing ,max_loss=0.35):\n",
    "\n",
    "    # Add Alpha Score to rest of the factors\n",
    "    alpha_score_label = '增强_ALPHA'\n",
    "    factors_with_alpha = data.loc[samples.index].copy()\n",
    "    factors_with_alpha[alpha_score_label] = alpha_score\n",
    "    \n",
    "    # Setup data for AlphaLens\n",
    "    print('Cleaning Data...\\n')\n",
    "    factor_data = build_factor_data(factors_with_alpha[factors + [alpha_score_label]], \n",
    "                                    pricing,max_loss=max_loss,periods = [10])\n",
    "    print('\\n-----------------------\\n')\n",
    "    \n",
    "    # Calculate Factor Returns and Sharpe Ratio\n",
    "    factor_returns = project_helper.get_factor_returns(factor_data)\n",
    "    sharpe_ratio = project_helper.sharpe_ratio(factor_returns,annualization_factor=np.sqrt(26))\n",
    "    \n",
    "    # Show Results\n",
    "    print('             Sharpe Ratios')\n",
    "    print(sharpe_ratio.round(2))\n",
    "    project_helper.plot_factor_returns(factor_returns,period=10)\n",
    "    project_helper.plot_factor_rank_autocorrelation(factor_data)\n",
    "    \n",
    "    factor_IC = get_IC(factor_data)\n",
    "    print(factor_IC.mean())\n",
    "    return factor_IC\n",
    "\n",
    "\n",
    "X_Final, y_Final= Get_Date(X, y, '2019-07-26T00:00:00.000000000')\n",
    "factor_IC = show_sample_results(all_factors, X_Final, factor_names,alpha_score = AlphaScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T19:02:01.575944Z",
     "start_time": "2021-01-24T19:02:01.567965Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"AlphaScores_10D_M.npy\",AlphaScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:27:53.284008Z",
     "start_time": "2021-01-13T07:27:53.078558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435382"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reset_index()['level_0'].drop_duplicates().loc[X.reset_index()['level_0'].drop_duplicates() == '2019-07-26', ].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T09:10:43.985267Z",
     "start_time": "2021-01-24T09:10:43.978124Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AlphaScores = np.load(\"AlphaScores.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
